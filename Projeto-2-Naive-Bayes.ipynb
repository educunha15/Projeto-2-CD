{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 2 - Classificador Autom√°tico de Sentimento\n",
    "\n",
    "Professor: Raul Ikeda\n",
    "\n",
    "Alunos: Rafael da Fonte e Eduardo Cunha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Programa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Importando os dois dataframes\n",
    "\n",
    "excel = pd.ExcelFile('tweets_airpods_201809062235.xlsx')\n",
    "data_treinamento = pd.read_excel(excel, 'Treinamento')\n",
    "data_teste = pd.read_excel(excel, 'Teste')\n",
    "\n",
    "# Limpando as colunas Treinamento e Teste dos dataframes\n",
    "\n",
    "treinamento = data_treinamento['Treinamento'].str.replace('@', '')\n",
    "treinamento = treinamento.str.replace('rt', '')\n",
    "treinamento = treinamento.str.replace(',', '')\n",
    "treinamento = treinamento.str.replace('.', '')\n",
    "treinamento = treinamento.str.replace(':', '')\n",
    "treinamento = treinamento.str.replace(';', '')\n",
    "treinamento = treinamento.str.replace('\\n', '')\n",
    "treinamento = treinamento.str.replace('\"', '')\n",
    "treinamento = treinamento.str.replace('‚Äú', '')\n",
    "treinamento = treinamento.str.replace('‚Äù', '')\n",
    "treinamento = treinamento.str.replace('$', ' $ ')\n",
    "treinamento = treinamento.str.replace('!', ' ! ')\n",
    "treinamento = treinamento.str.replace('?', ' ? ')\n",
    "\n",
    "teste = data_teste['Teste'].str.replace('@', '')\n",
    "teste = teste.str.replace('rt', '')\n",
    "teste = teste.str.replace(',', '')\n",
    "teste = teste.str.replace('.', '')\n",
    "teste = teste.str.replace(':', '')\n",
    "teste = teste.str.replace(';', '')\n",
    "teste = teste.str.replace('\\n', '')\n",
    "teste = teste.str.replace('\"', '')\n",
    "teste = teste.str.replace('‚Äú', '')\n",
    "teste = teste.str.replace('‚Äù', '')\n",
    "teste = teste.str.replace('‚Äú', '')\n",
    "teste = teste.str.replace('‚Äù', '')\n",
    "teste = teste.str.replace('$', ' $ ')\n",
    "teste = teste.str.replace('!', ' ! ')\n",
    "teste = teste.str.replace('?', ' ? ')\n",
    "\n",
    "# Inserindo as colunas limpas em seus respectivos dataframes\n",
    "\n",
    "data_treinamento['Treinamento'] = treinamento\n",
    "data_teste['Teste'] = teste\n",
    "\n",
    "# Selecionando os relevantes e irrelevantes da base de treinamento\n",
    "\n",
    "Relevantes = data_treinamento[data_treinamento['PRODUTO : APPLE AIRPODS'] == 'Relevante']\n",
    "Irrelevantes = data_treinamento[data_treinamento['PRODUTO : APPLE AIRPODS'] == 'Irrelevante']\n",
    "\n",
    "# Definindo P(Relevante) e P(Irrelevante)\n",
    "\n",
    "PRelevante = len(Relevantes)/len(data_treinamento)\n",
    "PIrrelevante = len(Irrelevantes)/len(data_treinamento)\n",
    "\n",
    "# Comparando P(Relevante|tweet) com P(Irrelevante|tweet)\n",
    "# P(Relevante|tweet) = P(tweet|Relevante) x P(Relevante)\n",
    "# P(Irrelevante|tweet) = P(tweet|Irrelevante) x P(Irrelevante)\n",
    "\n",
    "# Criando as listas com tweets, sendo cada um uma lista com as palavras separadas\n",
    "\n",
    "lista_tweets = []\n",
    "lista_tweets_relevantes = []\n",
    "lista_tweets_irrelevantes = []\n",
    "\n",
    "for tweet in Relevantes['Treinamento']:\n",
    "    lista_tweets_relevantes.append(tweet.split())\n",
    "    \n",
    "for tweet in Irrelevantes['Treinamento']:\n",
    "    lista_tweets_irrelevantes.append(tweet.split())\n",
    "\n",
    "for tweet in treinamento:\n",
    "    lista_tweets.append(tweet.split())\n",
    "    \n",
    "# Verificando palavras possiveis para Laplace Smoothing\n",
    "\n",
    "palavras_possiveis = []\n",
    "\n",
    "for tweet in lista_tweets:\n",
    "    for palavra in tweet:\n",
    "        if palavra.lower() not in palavras_possiveis:\n",
    "            palavras_possiveis.append(palavra.lower())\n",
    "            \n",
    "# Calcula P(palavra|Relevante) com Laplace Smoothing    \n",
    "    \n",
    "def ProbRelevante(palavra):\n",
    "    ocorrencia = 1\n",
    "    total_de_palavras = 0\n",
    "    \n",
    "    for tweet in lista_tweets_relevantes:\n",
    "        total_de_palavras += len(tweet)\n",
    "        \n",
    "        for i in tweet:\n",
    "            if i.lower() == palavra.lower():\n",
    "                ocorrencia += 1\n",
    "                \n",
    "    return ocorrencia/(total_de_palavras + len(palavras_possiveis))\n",
    "\n",
    "# Calcula P(palavra|Irrelevante) com Laplace Smoothing\n",
    "\n",
    "def ProbIrrelevante(palavra):\n",
    "    ocorrencia = 1\n",
    "    total_de_palavras = 0\n",
    "    \n",
    "    for tweet in lista_tweets_irrelevantes:\n",
    "        total_de_palavras += len(tweet)\n",
    "        \n",
    "        for i in tweet:\n",
    "            if i.lower() == palavra.lower():\n",
    "                ocorrencia += 1\n",
    "    \n",
    "    return ocorrencia/(total_de_palavras + len(palavras_possiveis))\n",
    "\n",
    "# Classificando os tweets da base de treinamento\n",
    "\n",
    "def classifica(lista_de_tweets):\n",
    "    classificacao = []\n",
    "\n",
    "    for tweet in lista_de_tweets: \n",
    "        tweet_s = tweet.split()\n",
    "    \n",
    "        PPRelevantes = []            \n",
    "        PPIrrelevantes = []\n",
    "        for palavra in tweet_s:\n",
    "            if palavra[0:5] != 'https':\n",
    "                PPRelevantes.append(ProbRelevante(palavra))\n",
    "                PPIrrelevantes.append(ProbIrrelevante(palavra))\n",
    "\n",
    "        PTweetRelevante = 1\n",
    "        PTweetIrrelevante = 1\n",
    "\n",
    "        for i in PPRelevantes:\n",
    "            PTweetRelevante *= i\n",
    "\n",
    "        for i in PPIrrelevantes:\n",
    "            PTweetIrrelevante *= i\n",
    "\n",
    "        PRelevanteTweet = PTweetRelevante * PRelevante\n",
    "        PIrrelevanteTweet = PTweetIrrelevante * PIrrelevante\n",
    "\n",
    "        if PRelevanteTweet > PIrrelevanteTweet:\n",
    "            classificacao.append('Relevante')\n",
    "        else:\n",
    "            classificacao.append('Irrelevante')\n",
    "    return classificacao\n",
    "\n",
    "lista_tweets_teste = []\n",
    "for tweet in teste:\n",
    "    lista_tweets_teste.append(tweet)\n",
    "\n",
    "classificacao = classifica(lista_tweets_teste)\n",
    "\n",
    "gabarito = data_teste.rename(index=str, columns={'Unnamed: 1':'Relevancia'}).Relevancia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Verificando a performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negativo Falso: 5.5%\n",
      "Positivo Falso: 15.5%\n",
      "Negativo Verdadeiro: 73.0%\n",
      "Positivo Verdadeiro: 6.0%\n"
     ]
    }
   ],
   "source": [
    "falpos = 0\n",
    "falneg = 0\n",
    "verpos = 0\n",
    "verneg = 0\n",
    "\n",
    "for i in range(len(gabarito)):\n",
    "    if gabarito[i] == classificacao[i] and gabarito[i] == 'Relevante':\n",
    "        verpos += 1\n",
    "    elif gabarito[i] == classificacao[i] and gabarito[i] == 'Irrelevante':\n",
    "        verneg += 1\n",
    "    elif gabarito[i] != classificacao[i] and gabarito[i] == 'Relevante' and classificacao[i] == 'Irrelevante':\n",
    "        falneg += 1\n",
    "    else:\n",
    "        falpos += 1\n",
    "\n",
    "tweets_classificados_relevantes = []\n",
    "\n",
    "for i in range(len(classificacao)):\n",
    "    if classificacao[i] == \"Relevante\":\n",
    "        tweets_classificados_relevantes.append(lista_tweets_teste[i])\n",
    "        \n",
    "falpos /= len(classificacao)\n",
    "falneg /= len(classificacao)\n",
    "verpos /= len(classificacao)\n",
    "verneg /= len(classificacao)\n",
    "\n",
    "#print(verpos+verneg+falpos+falneg)\n",
    "\n",
    "print(\"Negativo Falso: {}%\".format(falneg*100))\n",
    "print(\"Positivo Falso: {}%\".format(falpos*100))\n",
    "print(\"Negativo Verdadeiro: {}%\".format(verneg*100))\n",
    "print(\"Positivo Verdadeiro: {}%\".format(verpos*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweets classificados como relevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple airpodsany good ?  these huawei freebuds are cack\n",
      "after these new airpods / cases drop i'm making it a task to remove all plug in chargers from the crib\n",
      "20 mos it'd taken to learn what kind of ux was hidden beneath that case o cruel misunderstanding !  o stubborn self-willed exile from the loving breast !  but it was all right everything was all right the struggle was finished he had won the victory over himself he loved airpods https//tco/emqjnipd0c\n",
      " tokenasty i just spent 10 minutes looking for my airpods already in my ears wtf\n",
      "sfmta_muni castro station i dropped my  $  airpods on inbound tracks between 1st/2nd benches  guessing i shouldn't get them myself ?   can u save my day ?   keep losing the damn things argh ! \n",
      "it‚Äôs hard to keep up with airpods them hoes stay falling out\n",
      "how you manage work airpods into the tweet bost ma brain üëèüèæ https//tco/hddk6atli2\n",
      "gorgeousahar you damn right / even though i purchased them once after the main ones broke i knew it was gonna be a bad investment pray for me cause i plan to invest in those airpods but if anything happens istg\n",
      " zhusalsa pcb assembly machine !  looking forward to the pcb order #raspberrypi #circuitboards  #electricalengineering #electronics #diye‚Ä¶\n",
      "yo how tf you work these airpods üòë\n",
      "mendescrewinfo shawnmendes i‚Äôm betting he already lost his airpods üòÇ\n",
      " laurensanderson the fact that i've watched tana's videos since forever ago and am now being listened to in her airpods is truly impecc‚Ä¶\n",
      "_roastduck wait there are black airpods now !  ? &gt\n",
      " eargo request your free sample of what slashgear calls the airpods of hearing aids to learn how eargo's devices fit and feel htt‚Ä¶\n",
      "the only reason i keep this guy around is bc he has money and he‚Äôs considering buying me airpods but other than that he‚Äôs so boring help\n",
      "beach day !  ‚òÄÔ∏è‚òÄÔ∏èüòéüòé #levantebeach #spain #benidorm #beach #beachday #holiday #sunbathing #suntan #catchingrays #airpods #rayban #sunglasses #andrewchristian #andrewchristianswimwear‚Ä¶ https//tco/l1sffwiv6h\n",
      "i may look like a tool in these new #airpods but i feel official af !  and the efficiency is unparalleled and i have a choice sheen on the top of my dome piece #beeninthebustoolong‚Ä¶ https//tco/lmzgesqoaz\n",
      " chris_daughtry i may look like a tool in these new #airpods but i feel official af !  and the efficiency is unparalleled and i have a c‚Ä¶\n",
      " waltmossberg mims pierce i love my iphone x even after nearly a year it feels new i love my ipad pro i rely on my apple watch b‚Ä¶\n",
      "qui vends des airpods\n",
      "whoever found my airpods at the stephanie eos parking lot merry christmas üéÖüèº\n",
      "airpods are literally the best thing ever i can leave me phone downstairs and still listen to music when i‚Äôm upstairs üéâ\n",
      "valkyri06801156 _xlavanya why doesn‚Äôt he even own airpods and make you struggle with terminology ?  !  what a jerk\n",
      " chris_daughtry i may look like a tool in these new #airpods but i feel official af !  and the efficiency is unparalleled and i have a c‚Ä¶\n",
      "please don‚Äôt most the time my airpods died and i just have them in specifically to avoid unwanted conversations https//tco/m79qpjemoc\n",
      " jamiep200 airpods are the only apple product i recommend buying https//tco/wmjvm3402i\n",
      "üî•¬†üî•¬†üî•apple mmef2am/a airpods wireless bluetooth headset for iphon¬† only  $ 14498üî•¬†üî•¬†üî• save 9% today !  click here https//tco/9ewmv2puua flys gfxcoach promotegamersdemented_s mighty_s lazy_s buzztwinblog turbo_s ttules_ssgh_s https//tco/99k0ggz8re\n",
      "i just had my airpods in listening to classical music on full blast and didn‚Äôt even notice my professor staed teaching\n",
      " techinrl airpods with android !  https//tco/imzq6lowsz #airpods #android #samsunggalaxynote9 #galaxynote9 #galaxys9 #pixel2xl #pixel2‚Ä¶\n",
      " eargo request your free sample of what slashgear calls the airpods of hearing aids to learn how eargo's devices fit and feel htt‚Ä¶\n",
      " waltmossberg mims pierce i love my iphone x even after nearly a year it feels new i love my ipad pro i rely on my apple watch b‚Ä¶\n",
      " chris_daughtry i may look like a tool in these new #airpods but i feel official af !  and the efficiency is unparalleled and i have a c‚Ä¶\n",
      "these airpods so amazing bruh\n",
      "rishissb have airpods they're dope expensive but great if you can afford em probably 8 hours of life in the charging pack maybe more with 30 minute breaks for the actual pods to fully charge sound quality is ight super convenient\n",
      "nothing like a hard run up candymountain #highehan1250elevation #pushing #burnin #ifitness #iampossible #adidas #airpods #applewatch #ifitness #jesusup #heismystrength  candy mountain‚Ä¶ https//tco/ottwpc0kwo\n",
      "what‚Äôs in my bag back to school edition üë©‚Äçüè´apple airpods ‚úÖfurry keychain ‚úÖeos egg lip balm ‚úÖgreen juice ‚úÖpaper ‚ùépens ‚ùé\n",
      "i love how fast these airpods charge\n",
      "i keep refreshing my email waiting to see that delivery email from apple about my airpods\n",
      " xxxmeezy i love my new airpods !  !  !  !  !  ! \n",
      " kvmagic - taehyung putting his airpods inside jungkook's ear !  !  !  cute babies listening to music backstage everyone say thank you japan‚Ä¶\n",
      "i hate traffic with no phone service even worse nigga this is boof i don‚Äôt have my goddamn airpods\n",
      "airpods were a great purchase highly recommend\n",
      "what to expect from next week‚Äôs big iphone event #appleevent 9/12/18 two sizes ‚Äî 58 and 65 inches referred to as the #iphonexs and #iphonexsplus a cheaper #iphonex the #applewatch series 4 #homepodmini a new #macbookair #airpower #airpods with water resistance #ios12 ‚åöÔ∏èüì±üíª\n"
     ]
    }
   ],
   "source": [
    "for i in tweets_classificados_relevantes:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Concluindo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    O classificador foi capaz de julgar corretamente a relev√¢ncia de 79% dos tweets. Dos 21% classificados erroneamente, 15.5% foram marcados como relevantes sendo na verdade irrelevantes, enquanto 5.5% foram marcados como irrelevantes sendo na verdade relevantes. Isso significa que caso de fato utilize-se esse classificador para obter informa√ß√µes do produto, 82,5% da informa√ß√£o irrelevante ser√° filtrada corretamente. Em contrapartida, 47,8% dos tweets relevantes ser√£o classificados como irrelevantes. Caso uma equipe de marketing analisasse todos os tweets testados, apenas 11,5% deles seriam relevantes. Com o uso do filtro do classificador, 27,9% dos tweets seriam relevantes. Assim sendo, mesmo que quase metade dos tweets relevantes n√£o sejam detectados pelo classificador, a chance de um tweet ser relevante ap√≥s a classifica√ß√£o quase triplica. Dessa forma, com o aumento da base de an√°lise, a efici√™ncia da equipe que anaisa os tweets aumenta, tendo em vista que um maior percentual do conte√∫do analisado ser√° de fato relevante.\n",
    "    \n",
    "    Com rela√ß√£o a mensagens com dupla nega√ß√£o ou sarcasmo, o classficador se mostra ineficiente por tratar cada palavra individualmente e n√£o levar o contexto ou outras palavras do tweet em considera√ß√£o na avalia√ß√£o individual.\n",
    "    Assim como j√° explanado, ap√≥s o uso do classificador, a probalbilidade de um tweet a ser analisado pela equipe de marketing ser relevante, aumenta em quase 250%, otimizando assim a an√°lise. Entretanto, viu-se tamb√©m que grande parte dos tweets relevantes foram perdidos e portanto, uma melhora no programa se mostra necess√°ria. Tal evolu√ß√£o se tornaria poss√≠vel atrav√©s do aumento da base de treinamento, tornando mais precisa a probabilidade calculada de cada palavra, bem como a implementa√ß√£o da associa√ß√£o de palavras no c√°lculo das probabilidades (como por exemplo associar o \"n√£o\" a palavra seguinte de modo a diferenciar a probabilidade de \"n√£o\" vezes a de \"gosto\", da probabiliade de ocorrer \"n√£o gosto\").\n",
    "    \n",
    "    Pode-se argumentar que agora que se possui um classificador funcional, esse programa poderia ser utilizado para aumentar a base de treinamento, supostamente aumentando o √≠ndice de acerto. Para explicar porque isso √© uma p√©ssima ideia, pode-se fazer uma analogia universit√°ria. Digamos que um aluno do curso de engenharia do Insper decide ver uma palestra sobre contabilidade para engenheiros. Alimentar a base de treinamento com o pr√≥prio programa seria como pedir para esse aluno dar uma aula de um semestre para uma sala de alunos do curso de economia. Ou seja, Uma pessoa que sabia pouco sobre o assunto est√° ensinando a um grupo grande de pessoas que sabem menos ainda. Analogamente, o classificador que j√° possui um √≠ndice de erro grande, ir√° apenas propagar o erro atrav√©s dessa estrat√©gia.\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
